{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from brian2 import *\n",
    "from brian2hears import *\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Stereo Audio Files to Mono PCM 16 Wav And Pad to 4 Seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absoluteFilePaths(directory):\n",
    "    file_paths = []\n",
    "    for folder, subfolders, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            filePath = os.path.abspath(os.path.join(folder, file)).replace('//','/')\n",
    "            file_paths.append(filePath)\n",
    "    return file_paths\n",
    "\n",
    "paths = absoluteFilePaths('data/urbansound8k/')\n",
    "\n",
    "for path in absoluteFilePaths('data/urbansound8k/'):\n",
    "    fold = os.path.basename(os.path.dirname(path))\n",
    "    slice_name = os.path.basename(path)\n",
    "\n",
    "    sr = librosa.get_samplerate(path)\n",
    "    y, sr = librosa.load(path,sr=sr,mono=True)\n",
    "    total_samples_with_pad = sr*4\n",
    "    y = librosa.util.fix_length(y,size=total_samples_with_pad)\n",
    "\n",
    "    export_path = 'data/urbansound8k_mono/' + fold + '/' + slice_name\n",
    "    print(export_path)\n",
    "    sf.write(export_path,y,sr,'PCM_16')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cochleagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cgram(filepath,cfN = 224, num_frames = 224):\n",
    "\n",
    "    sound = Sound(filepath)\n",
    "    \n",
    "    cf = erbspace(20*Hz,20000*Hz,cfN)\n",
    "    gammatone = Gammatone(sound,cf)\n",
    "    cochlea = FunctionFilterbank(gammatone, lambda x: clip(x,0,Inf)**(1/3))\n",
    "    output = cochlea.process()\n",
    "\n",
    "    samples_length = np.shape(output)[0]\n",
    "\n",
    "    N = samples_length\n",
    "\n",
    "    frame_length = int((N / (0.5*num_frames+0.5)))\n",
    "    hop_length = int(frame_length/2)\n",
    "\n",
    "    frames = librosa.util.frame(output.T,frame_length=frame_length,hop_length=hop_length)\n",
    "\n",
    "    while shape(frames)[2] != num_frames:\n",
    "        frames = frames[:,:,:-1]\n",
    "\n",
    "    windowed_frames = np.absolute(frames)\n",
    "    summed_energy = np.sum(windowed_frames,axis=1)\n",
    "    cochleagram_data = summed_energy+1e-12\n",
    "    cochleagram_data = np.absolute(cochleagram_data)*hamming(num_frames)\n",
    "    return cochleagram_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Gammachirp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_LinGC(filepath,cfN=224,num_frames=224):\n",
    "    sound = Sound(filepath)\n",
    "\n",
    "    #center frequencies with a spacing following an ERB scale\n",
    "    center_frequencies = erbspace(20*Hz, 20000*Hz, cfN)\n",
    "\n",
    "    c = 0 #glide slope\n",
    "    time_constant = linspace(3, 0.3, cfN)*ms\n",
    "\n",
    "    gamma_chirp = LinearGammachirp(sound, center_frequencies, time_constant, c)\n",
    "    gamma_chirp = FunctionFilterbank(gamma_chirp, lambda x: clip(x,0,Inf)**(1/3))\n",
    "\n",
    "    output = gamma_chirp.process()\n",
    "\n",
    "    samples_length = np.shape(output)[0]\n",
    "    N = samples_length\n",
    "\n",
    "    frame_length = int((N / (0.5*num_frames+0.5)))\n",
    "    hop_length = int(frame_length/2)\n",
    "\n",
    "    frames = librosa.util.frame(output.T,frame_length=frame_length,hop_length=hop_length)\n",
    "\n",
    "    while shape(frames)[2] != num_frames:\n",
    "        frames = frames[:,:,:-1]\n",
    "\n",
    "    windowed_frames = np.absolute(frames)\n",
    "    summed_energy = np.sum(windowed_frames,axis=1)\n",
    "    LinGC_data = summed_energy+1e-12\n",
    "    LinGC_data = np.absolute(LinGC_data)*hamming(num_frames)\n",
    "\n",
    "    return LinGC_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithmic Gammachirp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_LogGC(filepath, cfN = 224, num_frames = 224, c1 = -2.96, b1 = 1.81):\n",
    "\n",
    "    #c1 - glide slope\n",
    "    #b1 - factor determining time constant of the filters\n",
    "\n",
    "    sound = Sound(filepath)\n",
    "\n",
    "    cf = erbspace(20*Hz, 20000*Hz, cfN) # centre frequencies\n",
    "\n",
    "    fb = LogGammachirp(sound, cf, c=c1, b=b1)\n",
    "    cochlea = FunctionFilterbank(fb, lambda x: clip(x,0,Inf)**(1/3))\n",
    "    output = cochlea.process()\n",
    "\n",
    "    samples_length = np.shape(output)[0]\n",
    "\n",
    "    N = samples_length\n",
    "\n",
    "    frame_length = int((N / (0.5*num_frames+0.5)))\n",
    "    hop_length = int(frame_length/2)\n",
    "\n",
    "    frames = librosa.util.frame(output.T,frame_length=frame_length,hop_length=hop_length)\n",
    "\n",
    "    while shape(frames)[2] != num_frames:\n",
    "        frames = frames[:,:,:-1]\n",
    "\n",
    "    windowed_frames = np.absolute(frames)\n",
    "    summed_energy = np.sum(windowed_frames,axis=1)\n",
    "    LogGC_data = summed_energy+1e-12\n",
    "    LogGC_data = np.absolute(LogGC_data)*hamming(num_frames)\n",
    "\n",
    "    return LogGC_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate Cochleagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_approxGT(filepath, cfN = 224, num_frames = 224):\n",
    "\n",
    "    sound = Sound(filepath)\n",
    "\n",
    "    cf = erbspace(20*Hz, 20000*Hz, cfN) # centre frequencies\n",
    "    \n",
    "    bw = 10**(0.037+0.785*log10(cf/Hz))\n",
    "\n",
    "    fb = ApproximateGammatone(sound, cf, bw, order=4)\n",
    "    cochlea = FunctionFilterbank(fb, lambda x: clip(x,0,Inf)**(1/3))\n",
    "    output = cochlea.process()\n",
    "\n",
    "    samples_length = np.shape(output)[0]\n",
    "\n",
    "    N = samples_length\n",
    "\n",
    "    frame_length = int((N / (0.5*num_frames+0.5)))\n",
    "    hop_length = int(frame_length/2)\n",
    "\n",
    "    frames = librosa.util.frame(output.T,frame_length=frame_length,hop_length=hop_length)\n",
    "\n",
    "    while shape(frames)[2] != num_frames:\n",
    "        frames = frames[:,:,:-1]\n",
    "\n",
    "    windowed_frames = np.absolute(frames)\n",
    "    summed_energy = np.sum(windowed_frames,axis=1)\n",
    "    approxGT_data = summed_energy+1e-12\n",
    "    approxGT_data = np.absolute(approxGT_data)*hamming(num_frames)\n",
    "\n",
    "    return approxGT_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of each representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to make the two columns separately for a 2x2 graph in my final paper\n",
    "# because matplotlib would not remove the excess horizontal spacing when attempting to do it\n",
    "# in the intended way -> f, axarr = plt.subplots(2,2)\n",
    "# I tried many different pad arguments to no avail\n",
    "\n",
    "soundpath = \"data/urbansound8k_mono/fold7/99812-1-2-0.wav\"\n",
    "\n",
    "cochleagram =  generate_cgram(soundpath)\n",
    "lin_gc = generate_LinGC(soundpath)\n",
    "log_gc = generate_LogGC(soundpath)\n",
    "approx_gt = generate_approxGT(soundpath)\n",
    "\n",
    "f, axarr = plt.subplots(2,1)\n",
    "axarr[0].imshow(cochleagram, origin='lower', vmin=0)\n",
    "axarr[0].set_title('Cochleagram')\n",
    "axarr[0].title.set_size(9)\n",
    "axarr[1].imshow(lin_gc, origin='lower', vmin=0)\n",
    "axarr[1].set_title('Linear Gammachirp')\n",
    "axarr[1].title.set_size(9)\n",
    "\n",
    "f.subplots_adjust(hspace=0.0)\n",
    "f.tight_layout()\n",
    "for ax in axarr.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "f.savefig('DataRepCol1.png', pad_inches=0, transparent=True)\n",
    "\n",
    "g, axarr = plt.subplots(2,1)\n",
    "axarr[0].imshow(approx_gt, origin='lower', vmin=0)\n",
    "axarr[0].set_title('Approximate Gammatone')\n",
    "axarr[0].title.set_size(9)\n",
    "axarr[1].imshow(log_gc, origin='lower', vmin=0)\n",
    "axarr[1].set_title('Logarithmic Gammachirp')\n",
    "axarr[1].title.set_size(9)\n",
    "\n",
    "g.subplots_adjust(hspace=0.0)\n",
    "g.tight_layout()\n",
    "for ax in axarr.flat:\n",
    "    ax.axis('off')\n",
    "g.savefig('DataRepCol2.png', pad_inches=0, transparent=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blacklisted Data Points - Incorrect PCM Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_csv(\"data/UrbanSound8K.csv\")\n",
    "blacklist = ['19007-4-0-0.wav','36429-2-0-6.wav','36429-2-0-7.wav', '36429-2-0-13.wav', '36429-2-0-14.wav', '36429-2-0-15.wav','36429-2-0-18.wav','36429-2-0-23.wav','88466-7-0-0.wav']\n",
    "meta_df[meta_df['slice_file_name'].isin(blacklist)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in metadata, set directory for converted mono sounds\n",
    "meta_df = pd.read_csv(\"data/UrbanSound8K.csv\")\n",
    "file_path = 'data/urbansound8k_mono'\n",
    "class_map = {'0' : 'air_conditioner', '1' : 'car_horn', '2' : 'children_playing', '3' : 'dog_bark', '4' : 'drilling', \n",
    "                 '5' : 'engine_idling', '6' : 'gun_shot', '7' : 'jackhammer', '8' : 'siren', '9' : 'street_music'}\n",
    "\n",
    "#remove corrupt fsIDs - 8723 data points remain\n",
    "meta_df = meta_df[meta_df.fsID != 19007]\n",
    "meta_df = meta_df[meta_df.fsID != 36429]\n",
    "meta_df = meta_df[meta_df.fsID != 88466]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Data Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I uncommented one of the four commented functions below before running this script\n",
    "# in order to create representations for each type, one at a time due to large runtime\n",
    "\n",
    "features = []\n",
    "count_records = len(meta_df.slice_file_name)\n",
    "j = 1\n",
    "\n",
    "for index, row in meta_df.iterrows():\n",
    "    \n",
    "    file_name = os.path.join(os.path.abspath(file_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    print(file_name + ' ' + str(j) + '/' + str(count_records))\n",
    "    class_label = row[\"classID\"]\n",
    "    fold = row[\"fold\"]\n",
    "    #data = generate_cgram(file_name, cfN = 224, num_frames = 224)\n",
    "    #data = generate_approxGT(file_name, cfN = 224, num_frames = 224)\n",
    "    #data = generate_LogGC(file_name, cfN = 224, num_frames = 224, c1 = -2.96, b1 = 1.81)\n",
    "    #data = generate_LinGC(file_name, cfN = 224, num_frames = 224)\n",
    "    features.append([data, class_label, fold])\n",
    "    j=j+1\n",
    "\n",
    "# Convert into a Pandas dataframe, save off into pickle file with label and fold \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label', 'fold'])\n",
    "featuresdf.to_pickle('LogGC_224_comp3.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a52d4c035417b05bd55a248d14e7821f3e49e2f328c6b3af0e0e5c45894566b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
