{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, transforms as T\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import copy\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 224\n",
    "img_width = img_height\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 20\n",
    "train_base = True\n",
    "optimizer_name = \"Adam\"\n",
    "learning_rate = 0.00005\n",
    "num_classes = 10\n",
    "\n",
    "#Checking the availability of a GPU\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Transform Functions (For Later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAddGaussNoise(object):\n",
    "    def __init__(self, input_size, mean=0.0, std=None, add_noise_probability=1.0):\n",
    "        assert isinstance(input_size, (int, tuple))\n",
    "        assert isinstance(mean, (int, float))\n",
    "        assert isinstance(std, (int, float)) or std is None\n",
    "        assert isinstance(add_noise_probability, (float))\n",
    "\n",
    "\n",
    "        if isinstance(input_size, int):\n",
    "            self.input_size = (input_size, input_size)\n",
    "        else:\n",
    "            assert len(input_size) == 2\n",
    "            self.input_size = input_size\n",
    "\n",
    "        self.mean = mean\n",
    "\n",
    "        if std is not None:\n",
    "            assert std > 0.0\n",
    "            self.std = std\n",
    "        else:\n",
    "            self.std = std\n",
    "\n",
    "        assert add_noise_probability > 0.0 and add_noise_probability <= 1.0\n",
    "        self.add_noise_prob = add_noise_probability\n",
    "\n",
    "\n",
    "    def __call__(self, spectrogram):\n",
    "        if np.random.random() > self.add_noise_prob:\n",
    "            return spectrogram\n",
    "\n",
    "        # set some std value \n",
    "        min_pixel_value = np.min(spectrogram)\n",
    "        if self.std is None:\n",
    "            std_factor = 0.03     # factor number \n",
    "        std = np.abs(min_pixel_value*std_factor)\n",
    "\n",
    "        # generate a white noise spectrogram\n",
    "        gauss_mask = np.random.normal(self.mean, \n",
    "                                    std, \n",
    "                                    size=self.input_size).astype('float32')\n",
    "        \n",
    "        # add white noise to the sound spectrogram\n",
    "        noisy_visual = spectrogram + gauss_mask\n",
    "\n",
    "        return noisy_visual\n",
    "\n",
    "class MyRightShift(object):\n",
    "    def __init__(self, input_size, width_shift_range, shift_probability=1.0):\n",
    "        assert isinstance(input_size, (int, tuple))\n",
    "        assert isinstance(width_shift_range, (int, float))\n",
    "        assert isinstance(shift_probability, (float))\n",
    "\n",
    "        if isinstance(input_size, int):\n",
    "            self.input_size = (input_size, input_size)\n",
    "        else:\n",
    "            assert len(input_size) == 2\n",
    "            self.input_size = input_size\n",
    "\n",
    "        if isinstance(width_shift_range, int):\n",
    "            assert width_shift_range > 0\n",
    "            assert width_shift_range <= self.input_size[1]\n",
    "            self.width_shift_range = width_shift_range\n",
    "        else:\n",
    "            assert width_shift_range > 0.0\n",
    "            assert width_shift_range <= 1.0\n",
    "            self.width_shift_range = int(width_shift_range * self.input_size[1])\n",
    "                        \n",
    "        assert shift_probability > 0.0 and shift_probability <= 1.0\n",
    "        self.shift_prob = shift_probability\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if np.random.random() > self.shift_prob:\n",
    "            return image\n",
    "\n",
    "        # create a new array filled with the min value\n",
    "        shifted_image= np.full(self.input_size, np.min(image), dtype='float32')\n",
    "\n",
    "        # randomly choose a start postion\n",
    "        rand_position = np.random.randint(1, self.width_shift_range)\n",
    "\n",
    "        # shift the image\n",
    "        shifted_image[:,rand_position:] = copy.deepcopy(image[:,:-rand_position])\n",
    "\n",
    "        return shifted_image\n",
    "\n",
    "#applying required transformations on the dataset\n",
    "img_transforms = {\n",
    "    'train':\n",
    "    T.Compose([\n",
    "        MyAddGaussNoise(input_size = img_height,add_noise_probability=0.5),\n",
    "        MyRightShift(input_size = img_height, width_shift_range=0.9, shift_probability=0.5),\n",
    "        T.ToTensor()\n",
    "        ]),\n",
    "\n",
    "    'valid':\n",
    "    T.Compose([\n",
    "        T.ToTensor()\n",
    "        ])\n",
    "     }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Dataset Object (UrbanSound8kDataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSound8kDataset(Dataset):\n",
    "    def __init__(self, featuresdf, transform=None):\n",
    "        assert isinstance(featuresdf, pd.DataFrame)\n",
    "        assert len(featuresdf.columns) == 3\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        self.featuresdf = featuresdf\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.featuresdf)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        cochleagram, label, fold = self.featuresdf.iloc[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "           cochleagram = self.transform(cochleagram)\n",
    "\n",
    "        if not torch.is_tensor(cochleagram):\n",
    "            cochleagram = torch.as_tensor(cochleagram.astype('float'))\n",
    "\n",
    "        label = torch.as_tensor(np.array(label)).type(torch.LongTensor)\n",
    "\n",
    "        cochleagram = cochleagram.expand(3,-1,-1).float()\n",
    "\n",
    "        return cochleagram, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Model Architecture Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the pretrained EfficientNet model\n",
    "def init_model():\n",
    "    model_transfer = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "    # Unfreeze weights\n",
    "    for param in model_transfer.parameters():\n",
    "        param.requires_grad = train_base\n",
    "    in_features = model_transfer._fc.in_features\n",
    "\n",
    "    # Defining Dense top layers after the convolutional layers\n",
    "    model_transfer._fc = nn.Sequential(   \n",
    "        nn.Linear(in_features, num_classes),\n",
    "        )\n",
    "\n",
    "    # selecting loss function\n",
    "    criterion_transfer = nn.CrossEntropyLoss()\n",
    "\n",
    "    #using Adam classifier\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer_transfer = optim.Adam(model_transfer.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        model_transfer = model_transfer.cuda()\n",
    "\n",
    "    return model_transfer, optimizer_transfer, criterion_transfer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the function for training\n",
    "def train_model(n_epochs, loaders, model, optimizer, criterion, fold_k, data_type, model_name, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    epochs = [] \n",
    "    trainingloss = []\n",
    "    validationloss = []\n",
    "    valaccuracy = []\n",
    "    learningrates = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize the variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "\n",
    "        preds = []\n",
    "        targets = []\n",
    "        \n",
    "        ###################\n",
    "        # training the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):     \n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()   \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "           \n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "        \n",
    "    \n",
    "        ######################    \n",
    "        # validating the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            pred = output.data.max(1,keepdim=True)[1]\n",
    "            preds.append(pred)\n",
    "            targets.append(target)\n",
    "            # compare predictions\n",
    "            correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "            total += data.size(0)\n",
    "        \n",
    "        train_loss = train_loss/len(train_ds)\n",
    "        valid_loss = valid_loss/len(val_ds)\n",
    "        valid_acc = correct / total\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        trainingloss.append(train_loss)\n",
    "        validationloss.append(valid_loss)\n",
    "        valaccuracy.append(valid_acc)\n",
    "        epochs.append(epoch)\n",
    "        learningrates.append(current_lr)\n",
    "\n",
    "        # printing training/validation statistics \n",
    "        print('Fold: {} \\tEpoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.6f}'.format(\n",
    "            fold_k,\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            valid_acc\n",
    "            ))\n",
    "        \n",
    "        ## saving the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            \n",
    "            valid_loss_min = valid_loss\n",
    "        \n",
    "    #GATHER TRAINING RESULTS IN DATAFRAME\n",
    "    validationloss = [i.cpu().tolist() for i in validationloss]\n",
    "    trainingloss = [i.cpu().tolist() for i in trainingloss]\n",
    "    zipped_data = list(zip(epochs, learningrates, trainingloss, validationloss, valaccuracy))\n",
    "    train_report = pd.DataFrame(zipped_data,columns=['Epoch','Learning Rate', 'Training Loss','Validation Loss','Validation Accuracy'])\n",
    "    train_report['Test Fold'] = fold_k\n",
    "    train_report['Model'] = model_name\n",
    "    train_report['Data Representation'] = data_type\n",
    "    train_report = train_report[['Model','Data Representation', 'Test Fold', 'Epoch','Learning Rate','Training Loss','Validation Loss','Validation Accuracy']]\n",
    "            \n",
    "    # return trained model\n",
    "    return model, train_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Script (Load Data From Disk, Train on All 10 Folds, Evaluate on Test Data, Generate Training and Test Metric Reports):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representation Name\n",
    "data_type = 'approxGT'\n",
    "\n",
    "# Model Name\n",
    "model_name = 'EfficientNet-b0'\n",
    "\n",
    "# Perform 10 fold validation\n",
    "for fold_k in range(1,num_classes+1):\n",
    "    featuresdf = pd.read_pickle('approxGT_224.pkl')\n",
    "    model_transfer, optimizer_transfer, criterion_transfer = init_model()\n",
    "    train_df = featuresdf[featuresdf['fold'] != fold_k]\n",
    "    val_df = featuresdf[featuresdf['fold'] == fold_k]\n",
    "    test_fold = fold_k\n",
    "\n",
    "    train_ds = UrbanSound8kDataset(train_df, transform=img_transforms['train'])\n",
    "    val_ds = UrbanSound8kDataset(val_df, transform=img_transforms['valid'])\n",
    "\n",
    "    #Creating loaders for the dataset\n",
    "    loaders_transfer={\n",
    "        'train':torch.utils.data.DataLoader(train_ds,batch_size,shuffle=True),\n",
    "        'valid':torch.utils.data.DataLoader(val_ds,batch_size,shuffle=False)\n",
    "    }\n",
    "\n",
    "    del featuresdf, train_df, val_df\n",
    "\n",
    "    if fold_k == 1:\n",
    "        #TRAIN THE MODEL AND SAVE RESULTS TO RESULTS_DF\n",
    "        train_report = train_model(n_epochs, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, fold_k, data_type, model_name, model_name + '_' + data_type + '_fold' + str(fold_k) + '.pt')[1]\n",
    "    else:\n",
    "        #TRAIN THE MODEL AND ADD DATA TO RESULTS DF\n",
    "        train_report_temp = train_model(n_epochs, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, fold_k, data_type, model_name, model_name + '_' + data_type + '_fold' + str(fold_k) + '.pt')[1]\n",
    "        train_report = pd.concat([train_report, train_report_temp])\n",
    "    \n",
    "    del model_transfer, optimizer_transfer, criterion_transfer\n",
    "    \n",
    "    #RELOAD FINAL CHECKPOINTED MODEL IN FOR VALIDATION RESULTS\n",
    "    model_transfer = init_model()[0]\n",
    "    model_transfer.load_state_dict(torch.load(model_name + '_' + data_type + '_fold' + str(fold_k) + '.pt'))\n",
    "    model_transfer.eval()\n",
    "\n",
    "    #PERFORM FINAL INFERENCE ON VALIDATION SET\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    class_names = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music']\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loaders_transfer['valid']):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        output = model_transfer(data)\n",
    "        prediction = torch.argmax(output, dim=1)\n",
    "        preds.append(prediction.cpu().numpy())\n",
    "        targets.append(target.cpu().numpy())\n",
    "\n",
    "    targets = np.concatenate(targets)\n",
    "    preds = np.concatenate(preds)\n",
    "\n",
    "    metrics_report_dict = classification_report(targets, preds, target_names=class_names, output_dict=True)\n",
    "    fold_acc_dict = {'Test Fold' : fold_k , 'Accuracy' : metrics_report_dict['accuracy']}\n",
    "    del metrics_report_dict['accuracy']\n",
    "\n",
    "    if fold_k == 1:\n",
    "        metrics_report = pd.DataFrame(metrics_report_dict).rename_axis('metric').reset_index()\n",
    "        fold_accuracies = pd.DataFrame(fold_acc_dict,index=[0])\n",
    "        metrics_report.insert(0,'Test Fold',fold_k)\n",
    "        metrics_report.insert(0,'Data Representation', data_type)\n",
    "        metrics_report.insert(0,'Model', model_name)\n",
    "    else:\n",
    "        metrics_report_temp = pd.DataFrame(metrics_report_dict).rename_axis('metric').reset_index()\n",
    "        fold_accuracies_temp = pd.DataFrame(fold_acc_dict,index=[0])\n",
    "        metrics_report_temp.insert(0,'Test Fold',fold_k)\n",
    "        metrics_report_temp.insert(0,'Data Representation', data_type)\n",
    "        metrics_report_temp.insert(0,'Model', model_name)\n",
    "        metrics_report = pd.concat([metrics_report, metrics_report_temp])\n",
    "        fold_accuracies = pd.concat([fold_accuracies, fold_accuracies_temp])\n",
    "\n",
    "    del data, target, output, prediction, targets, preds, metrics_report_dict, fold_acc_dict\n",
    "\n",
    "train_report.to_csv('results/TrainReportbyEpoch_' + model_name + '_' + data_type + '.csv', index=False)\n",
    "metrics_report.to_csv('results/TestMetricsbyClass_' + model_name + '_' + data_type + '.csv', index=False)\n",
    "fold_accuracies.to_csv('results/FoldAccuracies_' + model_name + '_' + data_type + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a52d4c035417b05bd55a248d14e7821f3e49e2f328c6b3af0e0e5c45894566b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
